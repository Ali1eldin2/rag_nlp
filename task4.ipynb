{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bee6f32",
   "metadata": {},
   "source": [
    "# DeepEval RAG Evaluation\n",
    "This notebook evaluates the existing Chroma-based RAG pipeline with DeepEval metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb9a3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DeepEval config directory ready: d:\\cellula_NLP\\task_rag\\.deepeval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anconda\\envs\\rag_eval\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anconda\\envs\\rag_eval\\lib\\site-packages\\deepeval\\__init__.py:49: UserWarning: You are using deepeval version 1.1.4, however version 3.7.0 is available. You should consider upgrading via the \"pip install --upgrade deepeval\" command.\n",
      "  warnings.warn(\n",
      "d:\\anconda\\envs\\rag_eval\\lib\\importlib\\__init__.py:127: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# Workaround for DeepEval permission issues\n",
    "# DeepEval tries to create a .deepeval directory in the current working directory\n",
    "# If you get permission errors, ensure you have write access to the current directory\n",
    "# or run the notebook from a location where you have write permissions\n",
    "\n",
    "# Store original working directory before any changes\n",
    "_ORIGINAL_CWD = os.getcwd()\n",
    "deepeval_dir = Path(\".deepeval\")\n",
    "\n",
    "try:\n",
    "    # Try to create and test write access to .deepeval directory\n",
    "    if not deepeval_dir.exists():\n",
    "        deepeval_dir.mkdir(exist_ok=True, mode=0o755)\n",
    "    # Test write permissions\n",
    "    test_file = deepeval_dir / \".test_write\"\n",
    "    test_file.write_text(\"test\")\n",
    "    test_file.unlink()\n",
    "    print(f\"✅ DeepEval config directory ready: {deepeval_dir.absolute()}\")\n",
    "except PermissionError:\n",
    "    # If we can't write to current directory, try user's temp directory\n",
    "    import tempfile\n",
    "    temp_base = Path(tempfile.gettempdir())\n",
    "    new_cwd = temp_base / \"deepeval_work\"\n",
    "    new_cwd.mkdir(exist_ok=True, mode=0o755)\n",
    "    os.chdir(new_cwd)\n",
    "    print(f\"⚠️  Changed working directory to: {new_cwd}\")\n",
    "    print(f\"   (Original: {_ORIGINAL_CWD})\")\n",
    "    print(\"   DeepEval will create .deepeval here instead.\")\n",
    "\n",
    "from deepeval.evaluate import evaluate\n",
    "from deepeval.metrics import (\n",
    "    ContextualPrecisionMetric,\n",
    "    ContextualRecallMetric,\n",
    "    ContextualRelevancyMetric,\n",
    "    FaithfulnessMetric,\n",
    "    AnswerRelevancyMetric,\n",
    ")\n",
    "from deepeval.models import GPTModel\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b3baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use absolute path for Chroma DB to avoid issues if working directory changes\n",
    "PERSIST_DIRECTORY = str(Path(_ORIGINAL_CWD) / \"chroma_db\")\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "GENERATION_MODEL = \"gpt-4o-mini\"\n",
    "DEEP_EVAL_MODEL = \"gpt-4o-mini\"\n",
    "RETRIEVE_TOP_K = 3\n",
    "\n",
    "# Workaround for langchain-openai version compatibility\n",
    "# If you get a 'proxies' error, try: pip install --upgrade langchain-openai openai\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RagQuery:\n",
    "    name: str\n",
    "    query: str\n",
    "    expected_answer: str\n",
    "    relevant_doc_ids: List[str]\n",
    "\n",
    "\n",
    "TEST_QUERIES: List[RagQuery] = [\n",
    "    RagQuery(\n",
    "        name=\"ai_capabilities\",\n",
    "        query=\"What can artificial intelligence systems do?\",\n",
    "        expected_answer=(\n",
    "            \"AI systems learn from experience, adapt to new inputs, and perform human-like tasks \"\n",
    "            \"such as perception, reasoning, planning, language understanding, and decision making.\"\n",
    "        ),\n",
    "        relevant_doc_ids=[\"ai_overview\"],\n",
    "    ),\n",
    "    RagQuery(\n",
    "        name=\"deep_learning_definition\",\n",
    "        query=\"Give a short definition of deep learning.\",\n",
    "        expected_answer=(\n",
    "            \"Deep learning is a machine learning approach that uses multi-layer neural networks to learn \"\n",
    "            \"hierarchical feature representations, enabling strong performance on tasks like vision, speech, and language.\"\n",
    "        ),\n",
    "        relevant_doc_ids=[\"deep_learning_intro\"],\n",
    "    ),\n",
    "    RagQuery(\n",
    "        name=\"training_process\",\n",
    "        query=\"How do neural networks learn during training?\",\n",
    "        expected_answer=(\n",
    "            \"Neural networks compare predictions to ground truth, compute loss, backpropagate errors, and update weights with gradient-based optimisation across many epochs.\"\n",
    "        ),\n",
    "        relevant_doc_ids=[\"neural_network_training\"],\n",
    "    ),\n",
    "    RagQuery(\n",
    "        name=\"ml_vs_dl\",\n",
    "        query=\"Differentiate machine learning and deep learning in one sentence.\",\n",
    "        expected_answer=(\n",
    "            \"Machine learning spans many algorithms including supervised and unsupervised methods, while deep learning relies on deep neural networks that learn end-to-end representations from raw inputs.\"\n",
    "        ),\n",
    "        relevant_doc_ids=[\"ml_vs_dl\"],\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ae8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_openai_key() -> None:\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise EnvironmentError(\n",
    "            \"OPENAI_API_KEY must be set in the environment before running DeepEval benchmarks.\\n\"\n",
    "            \"Set it using: os.environ['OPENAI_API_KEY'] = 'your-actual-api-key'\"\n",
    "        )\n",
    "    # Check if it's a placeholder\n",
    "    if \"your_key\" in api_key.lower() or \"here\" in api_key.lower() or len(api_key) < 20:\n",
    "        raise EnvironmentError(\n",
    "            f\"OPENAI_API_KEY appears to be a placeholder value: '{api_key[:10]}...'\\n\"\n",
    "            \"Please set a valid API key using: os.environ['OPENAI_API_KEY'] = 'sk-...'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def load_vectorstore() -> Chroma:\n",
    "    if not os.path.exists(PERSIST_DIRECTORY):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Chroma directory '{PERSIST_DIRECTORY}' not found. Run the indexing step before evaluation.\"\n",
    "        )\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "    return Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=embeddings)\n",
    "\n",
    "\n",
    "def build_rag_pipeline(vectorstore: Chroma):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": RETRIEVE_TOP_K})\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are a helpful assistant. Answer the user's question using only the provided context.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a concise answer that stays strictly faithful to the given context.\"\"\"\n",
    "    )\n",
    "\n",
    "    # Initialize ChatOpenAI - handle version compatibility issues\n",
    "    # The 'proxies' error is due to langchain-openai/openai version mismatch\n",
    "    # Try multiple initialization methods to find one that works\n",
    "    llm = None\n",
    "    init_methods = [\n",
    "        # Method 1: Standard initialization\n",
    "        lambda: ChatOpenAI(model=GENERATION_MODEL, temperature=0.2),\n",
    "        # Method 2: With model_name (older versions)\n",
    "        lambda: ChatOpenAI(model_name=GENERATION_MODEL, temperature=0.2),\n",
    "        # Method 3: With explicit openai_api_key\n",
    "        lambda: ChatOpenAI(\n",
    "            model=GENERATION_MODEL,\n",
    "            temperature=0.2,\n",
    "            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    for method in init_methods:\n",
    "        try:\n",
    "            llm = method()\n",
    "            break\n",
    "        except (TypeError, ValueError) as e:\n",
    "            if \"proxies\" in str(e):\n",
    "                # If proxies error persists, the issue is in the package versions\n",
    "                # User should run: pip install --upgrade langchain-openai openai\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    if llm is None:\n",
    "        raise RuntimeError(\n",
    "            \"Failed to initialize ChatOpenAI. This is likely due to a version mismatch. \"\n",
    "            \"Try running: pip install --upgrade langchain-openai openai\"\n",
    "        )\n",
    "    \n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    def run(question: str) -> Dict[str, Any]:\n",
    "        docs: List[Document] = retriever.invoke(question)\n",
    "        context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "        answer = (prompt | llm | parser).invoke({\"context\": context, \"question\": question})\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"docs\": docs,\n",
    "        }\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "def _doc_id(doc: Document) -> str:\n",
    "    metadata = getattr(doc, \"metadata\", {}) or {}\n",
    "    for key in (\"id\", \"source\", \"doc_id\", \"document_id\"):\n",
    "        if key in metadata:\n",
    "            return str(metadata[key])\n",
    "    if hasattr(doc, \"id\"):\n",
    "        return str(doc.id)\n",
    "    return str(abs(hash(doc.page_content)) % (10**16))\n",
    "\n",
    "\n",
    "def build_test_cases(generate_answer, queries: List[RagQuery]) -> List[LLMTestCase]:\n",
    "    test_cases: List[LLMTestCase] = []\n",
    "    for spec in queries:\n",
    "        result = generate_answer(spec.query)\n",
    "        docs: List[Document] = result[\"docs\"]\n",
    "        contexts = [doc.page_content for doc in docs]\n",
    "        retrieved_ids = [_doc_id(doc) for doc in docs]\n",
    "\n",
    "        test_cases.append(\n",
    "            LLMTestCase(\n",
    "                input=spec.query,\n",
    "                actual_output=result[\"answer\"],\n",
    "                expected_output=spec.expected_answer,\n",
    "                context=contexts,\n",
    "                retrieval_context=contexts,\n",
    "                additional_metadata={\n",
    "                    \"retrieved_doc_ids\": retrieved_ids,\n",
    "                    \"expected_doc_ids\": spec.relevant_doc_ids,\n",
    "                    \"query_name\": spec.name,\n",
    "                },\n",
    "            )\n",
    "        )\n",
    "    return test_cases\n",
    "\n",
    "\n",
    "def build_metrics() -> List[Any]:\n",
    "    judge_model = GPTModel(model=DEEP_EVAL_MODEL)\n",
    "    return [\n",
    "        ContextualPrecisionMetric(model=judge_model, threshold=0.5),\n",
    "        ContextualRecallMetric(model=judge_model, threshold=0.5),\n",
    "        ContextualRelevancyMetric(model=judge_model, threshold=0.5),\n",
    "        FaithfulnessMetric(model=judge_model, threshold=0.5),\n",
    "        AnswerRelevancyMetric(model=judge_model, threshold=0.5),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95596a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Please set your OPENAI_API_KEY in the cell above or in your environment variables\n"
     ]
    }
   ],
   "source": [
    "# Set your OpenAI API key here (if not already set in environment)\n",
    "# Replace 'your-api-key-here' with your actual OpenAI API key\n",
    "# You can get one from: https://platform.openai.com/account/api-keys\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\") or \"your_key\" in os.getenv(\"OPENAI_API_KEY\", \"\").lower():\n",
    "    \n",
    "    # Uncomment and set your API key below:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "    print(\"⚠️  Please set your OPENAI_API_KEY in the cell above or in your environment variables\")\n",
    "else:\n",
    "    print(\"✅ OPENAI_API_KEY is set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df94da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alieldin\\AppData\\Local\\Temp\\ipykernel_27148\\3164055969.py:22: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '.deepeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m rag_runner \u001b[38;5;241m=\u001b[39m build_rag_pipeline(vectorstore)\n\u001b[0;32m      4\u001b[0m test_cases \u001b[38;5;241m=\u001b[39m build_test_cases(rag_runner, TEST_QUERIES)\n\u001b[1;32m----> 5\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepared \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_cases)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m DeepEval test cases. Running evaluation...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m test_results \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m      9\u001b[0m     test_cases\u001b[38;5;241m=\u001b[39mtest_cases,\n\u001b[0;32m     10\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mmetrics,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     16\u001b[0m )\n",
      "Cell \u001b[1;32mIn[3], line 124\u001b[0m, in \u001b[0;36mbuild_metrics\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_metrics\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[1;32m--> 124\u001b[0m     judge_model \u001b[38;5;241m=\u001b[39m \u001b[43mGPTModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEEP_EVAL_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    126\u001b[0m         ContextualPrecisionMetric(model\u001b[38;5;241m=\u001b[39mjudge_model, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m    127\u001b[0m         ContextualRecallMetric(model\u001b[38;5;241m=\u001b[39mjudge_model, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m         AnswerRelevancyMetric(model\u001b[38;5;241m=\u001b[39mjudge_model, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m    131\u001b[0m     ]\n",
      "File \u001b[1;32md:\\anconda\\envs\\rag_eval\\lib\\site-packages\\deepeval\\models\\gpt_model.py:62\u001b[0m, in \u001b[0;36mGPTModel.__init__\u001b[1;34m(self, model, _openai_api_key, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anconda\\envs\\rag_eval\\lib\\site-packages\\deepeval\\models\\base_model.py:35\u001b[0m, in \u001b[0;36mDeepEvalBaseLLM.__init__\u001b[1;34m(self, model_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anconda\\envs\\rag_eval\\lib\\site-packages\\deepeval\\models\\gpt_model.py:65\u001b[0m, in \u001b[0;36mGPTModel.load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshould_use_azure_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     66\u001b[0m         openai_api_key \u001b[38;5;241m=\u001b[39m KEY_FILE_HANDLER\u001b[38;5;241m.\u001b[39mfetch_data(\n\u001b[0;32m     67\u001b[0m             KeyValues\u001b[38;5;241m.\u001b[39mAZURE_OPENAI_API_KEY\n\u001b[0;32m     68\u001b[0m         )\n\u001b[0;32m     70\u001b[0m         openai_api_version \u001b[38;5;241m=\u001b[39m KEY_FILE_HANDLER\u001b[38;5;241m.\u001b[39mfetch_data(\n\u001b[0;32m     71\u001b[0m             KeyValues\u001b[38;5;241m.\u001b[39mOPENAI_API_VERSION\n\u001b[0;32m     72\u001b[0m         )\n",
      "File \u001b[1;32md:\\anconda\\envs\\rag_eval\\lib\\site-packages\\deepeval\\models\\gpt_model.py:179\u001b[0m, in \u001b[0;36mGPTModel.should_use_azure_openai\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshould_use_azure_openai\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 179\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mKEY_FILE_HANDLER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKeyValues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUSE_AZURE_OPENAI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\anconda\\envs\\rag_eval\\lib\\site-packages\\deepeval\\key_handler.py:45\u001b[0m, in \u001b[0;36mKeyFileHandler.fetch_data\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetches the data from the hidden file\"\"\"\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKEY_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Handle the case when the file doesn't exist\u001b[39;00m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '.deepeval'"
     ]
    }
   ],
   "source": [
    "ensure_openai_key()\n",
    "vectorstore = load_vectorstore()\n",
    "rag_runner = build_rag_pipeline(vectorstore)\n",
    "test_cases = build_test_cases(rag_runner, TEST_QUERIES)\n",
    "metrics = build_metrics()\n",
    "\n",
    "print(f\"Prepared {len(test_cases)} DeepEval test cases. Running evaluation...\\n\")\n",
    "test_results = evaluate(\n",
    "    test_cases=test_cases,\n",
    "    metrics=metrics,\n",
    "    run_async=False,\n",
    "    show_indicator=True,\n",
    "    print_results=True,\n",
    "    write_cache=False,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Calculate and display metric pass rates\n",
    "from collections import defaultdict\n",
    "metric_counts = defaultdict(int)\n",
    "metric_successes = defaultdict(int)\n",
    "\n",
    "for result in test_results:\n",
    "    for metric_data in result.metrics_data:\n",
    "        metric_name = metric_data.name\n",
    "        metric_counts[metric_name] += 1\n",
    "        if metric_data.success:\n",
    "            metric_successes[metric_name] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Overall Metric Pass Rates\")\n",
    "print(\"=\" * 70)\n",
    "for metric_name in metric_counts:\n",
    "    pass_rate = metric_successes[metric_name] / metric_counts[metric_name]\n",
    "    print(f\"  {metric_name}: {pass_rate:.2%} ({metric_successes[metric_name]}/{metric_counts[metric_name]})\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4bb18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
